# ğŸ“š DocumentaciÃ³n para el Uso de Ollama

## ğŸ“‹ DescripciÃ³n del Proyecto

Este documento explica cÃ³mo usar **Ollama** para ejecutar modelos de lenguaje grandes (LLM) de manera local. Incluye ejemplos de cÃ³digo y comentarios para facilitar la integraciÃ³n.

---

## ğŸ› ï¸ Requisitos Previos

1. Tener Python 3.10+ instalado.
2. Crear una cuenta en Ollama y obtener tu API Key.
3. Instalar el paquete `ollama`:

```bash
pip install ollama
```

---

## ğŸš€ Primeros Pasos

### ğŸ”‘ Configurar la API Key

Primero, debes configurar tu API Key. Esto se hace a travÃ©s de una variable de entorno:

```bash
export OLLAMA_API_KEY="tu-api-key"
```

En Windows, usa:

```bash
set OLLAMA_API_KEY="tu-api-key"
```

---

## ğŸ“š Funcionalidades BÃ¡sicas

### ğŸ“ GeneraciÃ³n de Texto con Manejo de Errores

Puedes generar texto usando la funciÃ³n `chat()` para simular una conversaciÃ³n. AquÃ­ hay un ejemplo mÃ¡s completo con manejo de errores y comentarios para facilitar la comprensiÃ³n:

```python
import ollama

# Definir el prompt y el modelo
prompt = "Escribe un cÃ³digo de Python que imprima los nÃºmeros del 1 al 100 en una funciÃ³n."
modelo = "llama3.2"

try:
    # Enviar el mensaje al modelo
    response = ollama.chat(
        model=modelo,
        messages=[{'role': 'user', 'content': prompt}]
    )
    
    # Extraer y mostrar el contenido de la respuesta
    if 'message' in response and 'content' in response['message']:
        print("ğŸ“‹ Respuesta del Modelo:\n")
        print(response['message']['content'])
    else:
        print("âš ï¸ No se encontrÃ³ contenido en la respuesta.")

except Exception as e:
    print(f"âŒ Error al generar respuesta: {e}")
```

#### ğŸ§ª Prueba RÃ¡pida

Ejecuta el cÃ³digo anterior para verificar que tu configuraciÃ³n es correcta. DeberÃ­as recibir una respuesta generada por el modelo, o un mensaje de error en caso de que algo falle.

---

### ğŸ“‹ Entrenamiento Personalizado

Puedes entrenar tu propio modelo para respuestas personalizadas:

```python
import ollama

# Datos de entrenamiento personalizados
datos = [
    {"input": "Hola, Â¿cÃ³mo estÃ¡s?", "output": "Bien, gracias."},
    {"input": "Â¿CuÃ¡l es tu nombre?", "output": "Soy un modelo personalizado."}
]

# Entrenando el modelo
ollama.train(model_name="mi-modelo", data=datos)
```

#### ğŸ§ª Prueba de Entrenamiento

DespuÃ©s de entrenar el modelo, prueba con:

```python
prompt = "Â¿CÃ³mo estÃ¡s?"
modelo = "mi-modelo"

response = ollama.chat(model=modelo,
                       messages=[{'role': 'user', 'content': prompt}])

print(response['message']['content'])
```

---


